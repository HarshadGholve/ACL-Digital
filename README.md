## **Content**
- SQL Code Generation using PyTorch
- Dynamic Document Q&A App using RAG Pipeline

---

# **SQL Code Generation using PyTorch** ğŸ§ ğŸ’»  
This project focuses on generating SQL queries from natural language inputs using a Transformer-based model built with **PyTorch**. The solution leverages **PostgreSQL** for database interaction and follows a structured approach to data processing, model training, and prediction.

## **ğŸ“‚ Project Structure**  
```
SQL_Code_Generation/
â”‚
â”œâ”€â”€ data/                     # Dataset files (excluded from GitHub)
â”œâ”€â”€ src/                      # Source code
â”‚   â”œâ”€â”€ __init__.py           
â”‚   â”œâ”€â”€ data_processing.py    # Data loading and preprocessing
â”‚   â”œâ”€â”€ model.py              # Transformer model implementation
â”‚   â”œâ”€â”€ train.py              # Model training script
â”‚   â”œâ”€â”€ predict.py            # Query prediction script
â”‚   â””â”€â”€ utils.py              # Helper functions
â””â”€â”€ .gitignore                # Ignored large files (e.g., datasets)
```

## **ğŸš€ How to Run**  
### **1. Clone the Repository**  
```bash
git clone https://github.com/HarshadGholve/ACL-Digital.git
cd ACL-Digital/SQL_Code_Generation
```

### **2. Install Dependencies**  
```bash
pip install -r requirements.txt
```

### **3. Train the Model**  
```bash
python src/train.py
```

### **4. Make Predictions**  
Provide a natural language question to generate SQL queries:  
```bash
python src/predict.py "<your question here>"
```

### Example:  
```bash
python src/predict.py "List all restaurants with a rating above 4."
```

## **âš™ï¸ Requirements**  
- Python 3.10+  
- PyTorch  
- PostgreSQL  
- NLTK  
- Transformers  

## **ğŸ“¦ Dataset**  
The datasets are not included in the repository due to size constraints. You can download the datasets from **Kaggle**:  
â¡ï¸ [Yale University's SPIDER NLP Dataset](https://www.kaggle.com/datasets/jeromeblanchet/yale-universitys-spider-10-nlp-dataset/data)  

Place the downloaded datasets in the following path:  
```
data/raw/
```

---

# **Dynamic Document Q&A App using RAG Pipeline** ğŸ¤–ğŸ“š  
A **Retrieval-Augmented Generation (RAG)** project powered by **Streamlit**, **ChromaDB**, and **GroqAPI**, enabling users to ask questions based on uploaded documents or web links. The app retrieves relevant context and generates precise answers using **Llama3-8b-8192** and **HuggingFace text embeddings**.

## **ğŸ“‚ Project Structure**  
```
LangChain_RAG/
â”‚
â”œâ”€â”€ .env                    # Environment variables (API keys, configs)
â”œâ”€â”€ app.py                  # Main Streamlit application
â”œâ”€â”€ utils.py                # Utility functions for embedding and file handling
â”œâ”€â”€ vector_store.py         # ChromaDB setup and vector store interaction
â”œâ”€â”€ retrieval.py            # Document retrieval logic
â”œâ”€â”€ generation.py           # LLM-based answer generation
â”œâ”€â”€ components.py           # Streamlit UI components
â””â”€â”€ requirements.txt        # Dependencies
```

## **ğŸš€ How to Run the Project**  

### **1. Clone the Repository**  
```bash
git clone https://github.com/HarshadGholve/ACL-Digital.git
cd ACL-Digital/LangChain_RAG
```

### **2. Install Dependencies**  
```bash
pip install -r requirements.txt
```

### **3. Add API Keys**  
Create a `.env` file in the root directory and add your keys:  
```env
API_KEY=<your_groq_api_key>
LANGCHAIN_TRACING_V2=<your_langsmith_token>
```

### **4. Run the App**  
```bash
streamlit run app.py
```

## **ğŸŒŸ Features**  
- **Upload Documents**: Users can upload PDF, TXT, or CSV files as reference documents.  
- **Web Link Support**: Provide a URL to fetch relevant content for answering questions.  
- **Real-Time Q&A**: The app retrieves context from documents and generates concise answers using LLM.  
- **Persistent Vector Store**: Utilizes **ChromaDB** with DuckDB+Parquet for persistent storage.  
- **Interactive UI**: Built with **Streamlit** for a user-friendly experience.

## **ğŸ“¦ Requirements**  
- Python 3.10+  
- Streamlit  
- ChromaDB  
- GroqAPI  
- HuggingFace Transformers  
- Sentence-Transformers  

## **ğŸ’» Technologies Used**  
| **Component**        | **Technology**             | **Model Used**              |
|----------------------|----------------------------|-----------------------------|
| Frontend             | Streamlit                  | -                           |
| Vector Store         | ChromaDB                   | DuckDB+Parquet              |
| Chat Model           | GroqAPI                    | Llama3-8b-8192              |
| Embedding Model      | HuggingFace Transformers   | text-embedding-3-large      |
| Workflow Orchestration | LangGraph                | -                           |
| Tracing & Observability | LangSmith               | -                           |


ğŸ’¡ **Future Enhancements**  
- Multi-file uploads for complex queries.  
- Add caching for faster responses.  
- Human-in-the-loop feedback mechanism.

## **ğŸ‘¨â€ğŸ’» Author**  
**Harshad Gholve**  
- [GitHub](https://github.com/HarshadGholve)  
- [LinkedIn](https://www.linkedin.com/in/harshad-gholve/)  


---

# ğŸ™ï¸ **Real-Time Speech-to-Text App Using Groq API and Streamlit**

This project is a real-time speech-to-text application that leverages **Groq API** for high-accuracy transcription. Built using **Streamlit**, the app captures audio input from a microphone and converts it into text using various Groq-supported transcription models.

## ğŸ”§ **Features**
- Real-time speech recognition using **SpeechRecognition** and **Streamlit**.
- Integration with **Groq API** for advanced transcription models.
- Supports multiple transcription models with different accuracy and speed:
  - `distil-whisper-large-v3-en`: English-only transcription.
  - `whisper-large-v3-turbo`: Fast, multi-language transcription.
  - `whisper-large-v3`: High-accuracy, general-purpose transcription.

## ğŸ› ï¸ **Setup Instructions**
1. Clone the repository:
   ```bash
   git clone https://github.com/HarshadGholve/ACL-Digital.git
   cd ACL-Digital/Speech2Text
   ```
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Run the app:
   ```bash
   streamlit run app.py
   ```

## ğŸ“š **Usage**
1. Select a transcription model from the dropdown.
2. Click **Start Recording** to begin real-time speech transcription.
3. Click **Stop Recording** to end the session.
4. View the transcribed text in the **Final Output** section.

## ğŸ§° **Tech Stack**
- **Python**  
- **Streamlit**  
- **SpeechRecognition**  
- **Groq API**  

## ğŸ”‘ **API Key Setup**
Replace the placeholder API key with your **Groq API key** in the `api_key` parameter:
```python
client = Groq(api_key="your_api_key_here")
```

## ğŸš€ **Future Enhancements**
- Add language detection.
- Improve UI/UX.
- Implement error handling for better user experience.

---

Feel free to update the repository details and API key placeholders as needed! ğŸ˜Š
